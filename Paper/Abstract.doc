


 


Title
Model Free Imitation Learning with Policy Optimization
Jonathan Ho HOJ@CS.STANFORD.EDU

Abstract
 > Existing imitation learning algorithms involve solving a sequence of planning or reinforcement learning problem, and it is limited to be directly applicable to large high dimensional environments, and the performance can significantly degrade if the planning problems are not solved to optimality. 
> The method is based on policy gradients, and under the formalism of apprenticeship learning. Meanwhile, it is also a model-free algorithms.
> The approach scales to large continuous environments.
Introduction
Methods
Advantages
Disadvantages

The simplest approach to imitation learning is behavioral cloning.
Conceptually simple and theoretically sound
Cascading errors

Inverse Reinforcement Learning (IRL)
One of the most successful approaches to imitation learning, assume the behavior the learner desire to imitate is generated by an expert behaving optimally with respect to an unknown cost function. Do not suffer from cascading error actions.
IRL generalize expert behaviors. 
Great expensive

Contribution
Develop a gradients based optimization formulation over parameterized policies for apprenticeship learning.
Propose two model free realization of these optimization algorithms: standard policy gradients algorithm, and policy gradient algorithm that incorporates trust region constraints to stabilize optimization.
Preliminaries
>basic notation from reinforcement learning

> Stationary stochastic policies
State action value 
state visitation distribution

Apprenticeship learning
Basic idea
> Apprenticeship Learning is that the learner must find a policy that performs at least as well as the expert.
> The Apprenticeship Learning algorithm try to find a policy that minimizes the  
cost difference of policy computed from AL and expert policy.



> Two ingredients must be provided in order to instantiate the frame: cost function class (assume the cost function belongs to certain cost function), and optimization algorithm to solve the problem.


AL example


Feature expectation matching:
Define the cost class as a certain set of linear combination of these basic function:



solve the function by inverse reinforcement learning.

Game-theoretic approach:
proposed two AL algorithm: MWAL and LPAL



The weight constrains on the basis functions to line on the simplex allows the maximization over costs to be performed instead over a finite set.
Policy
 optimization
 for

 AL


The gradient-based optimization include two procedures, 1) fitting a local reinforcement learning problem to generate learning signal for imitation. 2) improving the policy with respect to this local problem.


Policy gradient
>Use the stochastic gradient descent
>Duo to the high variance incurred by stochastic gradient descent, the author adopt Trust Region Policy Optimization, a model free policy search algorithm capable of quickly training large neural network stochastic policies for complex tasks.

TRPO for RL
The Vanilla policy gradient methods to improve the policy:







The problem lie in the step size, how to improve the step size?



The optimization problem finally converted into 







TRPO for AL





Experiments
Evaluated the approach in a variety of scenarios: finite gridworls of varying sizes, the continuous planar navigation task, highway driving simulation
Future  
Work
 Generative adversarial networks (Goodfellow et al., 2014), the policy parameterizes
a generative model of state-action pairs, and the cost function serves as an adversary. 





